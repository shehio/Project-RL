<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Timelines on Reinforcement Learning Playbook</title>
        <link>https://rltimeline.com/timeline/</link>
        <description>Recent content in Timelines on Reinforcement Learning Playbook</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 13 Dec 2019 00:00:00 +0000</lastBuildDate>
        <atom:link href="https://rltimeline.com/timeline/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Dota 2 with Large Scale Deep Reinforcement Learning</title>
            <link>https://rltimeline.com/timeline/dota-rl-2019/</link>
            <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/dota-rl-2019/</guid>
            <description>Abstract   On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/DOTA.png" />
</figure>
<p>On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.</p>
<p><a href="https://arxiv.org/pdf/1912.06680.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
            <link>https://rltimeline.com/timeline/starcraft-rl-2019/</link>
            <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/starcraft-rl-2019/</guid>
            <description>Abstract   Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1-3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Alpha-Star.png" />
</figure>
<p>Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1-3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.</p>
<p><a href="https://www.nature.com/articles/s41586-019-1724-z">Nature Article</a>
<a href="https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Superhuman AI for multiplayer poker</title>
            <link>https://rltimeline.com/timeline/poker-rl-2019/</link>
            <pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/poker-rl-2019/</guid>
            <description>Abstract In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<p>In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold’em poker, the most popular form of poker played by humans.</p>
<p><a href="https://science.sciencemag.org/content/365/6456/885">Science Article</a>
<a href="https://www.cs.cmu.edu/~noamb/papers/19-Science-Superhuman.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
            <link>https://rltimeline.com/timeline/alpha-zero-2018/</link>
            <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/alpha-zero-2018/</guid>
            <description>Abstract   The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Alpha-Zero.png" />
</figure>
<p>The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world-champion program in the games of chess and shogi (Japanese chess), as well as Go.</p>
<p><a href="https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</title>
            <link>https://rltimeline.com/timeline/sac-2018/</link>
            <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/sac-2018/</guid>
            <description>Abstract   Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/SAC.png" />
</figure>
<p>Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.</p>
<p><a href="https://arxiv.org/pdf/1801.01290.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Mastering the game of Go without human knowledge</title>
            <link>https://rltimeline.com/timeline/self-play-alpha-go-zero-2017/</link>
            <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/self-play-alpha-go-zero-2017/</guid>
            <description>Abstract   A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Alpha-Go-Zero.png" />
</figure>
<p>A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo&rsquo;s own move selections and also the winner of AlphaGo&rsquo;s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.</p>
<p><a href="https://www.nature.com/articles/nature24270">Nature Article</a></p>
<p><a href="https://www.nature.com/articles/nature24270.epdf?sharing_token=Urd8Xo0dBqkq96ZG2bJAiNRgN0jAjWel9jnR3ZoTv0MzTglC12p1NiU2orOu-c1nBdYrrl5mO_G-4ivReqyPdGtpXIYZu2l7Mf8cQvvSKvsnA_tYG0ETemCRq4NAeEGa7s3Q5cxdhxdo4diqVNKfVv6z3pxjfJItf-_W8sMFzoEZgx9ZoMmgzTVWfy37nJ0Zel_NDqMt0C2d3XoGTl7rgX4DHsXdaqmx37M4elyLPpc%3D">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Proximal Policy Optimization Algorithms</title>
            <link>https://rltimeline.com/timeline/ppo-2017/</link>
            <pubDate>Mon, 28 Aug 2017 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/ppo-2017/</guid>
            <description>Abstract We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically).</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<p>We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.</p>
<p><a href="https://arxiv.org/pdf/1707.06347.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Hindsight Experience Replay</title>
            <link>https://rltimeline.com/timeline/hindsight-er-2016/</link>
            <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/hindsight-er-2016/</guid>
            <description>Abstract   Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.
We demonstrate our approach on the task of manipulating objects with a robotic arm.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/HER.png" />
</figure>
<p>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.</p>
<p>We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.</p>
<p><a href="https://arxiv.org/pdf/1707.01495.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Asynchronous Methods for Deep Reinforcement Learning</title>
            <link>https://rltimeline.com/timeline/asynchronous-methods-2016/</link>
            <pubDate>Thu, 04 Feb 2016 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/asynchronous-methods-2016/</guid>
            <description>Abstract   We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Asynchronous-methods.png" />
</figure>
<p>We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.</p>
<p><a href="https://arxiv.org/pdf/1602.01783.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Mastering the game of Go with deep neural networks and tree search</title>
            <link>https://rltimeline.com/timeline/mastering-go-2016/</link>
            <pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/mastering-go-2016/</guid>
            <description>Abstract       The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Mastering-GO-1.png" />
</figure>
<figure>
  <img src="/images/Mastering-GO-2.png" />
</figure>
<figure>
  <img src="/images/Mastering-GO-3.png" />
</figure>
<p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.</p>
<p><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Prioritized Experience Replay</title>
            <link>https://rltimeline.com/timeline/prioritized-er-2015/</link>
            <pubDate>Wed, 18 Nov 2015 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/prioritized-er-2015/</guid>
            <description>Abstract Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<p>Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.</p>
<p><a href="https://arxiv.org/pdf/1511.05952.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Deep Reinforcement Learning with Double Q-learning</title>
            <link>https://rltimeline.com/timeline/double-q-learning-2015/</link>
            <pubDate>Tue, 22 Sep 2015 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/double-q-learning-2015/</guid>
            <description>Abstract   The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Double-Q-learning.png" />
</figure>
<p>The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.</p>
<p><a href="https://arxiv.org/pdf/1509.06461.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Continuous control with deep reinforcement learning</title>
            <link>https://rltimeline.com/timeline/continuous-control-2015/</link>
            <pubDate>Wed, 09 Sep 2015 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/continuous-control-2015/</guid>
            <description>Abstract   We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Continuous-control.png" />
</figure>
<p>We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.</p>
<p><a href="https://arxiv.org/pdf/1509.02971.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Human-level control through deep reinforcement learning</title>
            <link>https://rltimeline.com/timeline/human-level-control-2015/</link>
            <pubDate>Thu, 26 Feb 2015 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/human-level-control-2015/</guid>
            <description>Abstract   The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behavior, of how agents may optimize their control of an environment. To use reinforcement learning successfully institutions approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Human-level-control.png" />
</figure>
<p>The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behavior, of how agents may optimize their control of an environment. To use reinforcement learning successfully institutions approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture, and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.</p>
<p><a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Trust Region Policy Optimization</title>
            <link>https://rltimeline.com/timeline/trpo-2015/</link>
            <pubDate>Thu, 19 Feb 2015 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/trpo-2015/</guid>
            <description>Abstract       We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/TRPO-1.png" />
</figure>
<figure>
  <img src="/images/TRPO-2.png" />
</figure>
<figure>
  <img src="/images/TRPO-3.png" />
</figure>
<p>We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.</p>
<p><a href="https://arxiv.org/pdf/1502.05477v5.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Deterministic Policy Gradient Algorithms</title>
            <link>https://rltimeline.com/timeline/dpg-2014/</link>
            <pubDate>Sat, 21 Jun 2014 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/dpg-2014/</guid>
            <description>Abstract   In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/DPG-2014.png" />
</figure>
<p>In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.</p>
<p><a href="http://proceedings.mlr.press/v32/silver14.pdf">Paper</a></p>
]]></content>
        </item>
        
        <item>
            <title>Playing Atari with Deep Reinforcement Learning</title>
            <link>https://rltimeline.com/timeline/atari-2013/</link>
            <pubDate>Thu, 19 Dec 2013 00:00:00 +0000</pubDate>
            
            <guid>https://rltimeline.com/timeline/atari-2013/</guid>
            <description>Abstract     We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm.</description>
            <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2>
<figure>
  <img src="/images/Atari-2013-1.png" />
</figure>
<figure>
  <img src="/images/Atari-2013-2.png" />
</figure>
<p>We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.</p>
<p><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Paper</a></p>
]]></content>
        </item>
        
    </channel>
</rss>
